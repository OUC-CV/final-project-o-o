## 项目报告

### 题目 Title

将HDR图像压缩为LDR图像的色调映射算法(ATT、TWT算法)**复现**

### 绪论 Introduction

#### 背景资料

尽管经过 20 年的研究和数十种算法的提出，高动态范围图像的色调映射仍然是一个难题。现实世界包含许多具有巨大动态范围的场景，但大多数图像格式仅使用 8 位存储亮度（像素值范围为0-255），并且大多数显示器同样受到限制。即使是具有扩展动态范围的显示器，也仍然无法再现世界上可见的亮度范围。高动态范围图像的映射一方面必须通过压缩将大的动态范围映射到显示设备的动态范围，这势必导致细节信息的丢失；另一方面为保持高亮与低暗的细节信息则需进行相应的增强，而这将导致**动态范围的压缩与细节增强间的矛盾**，容易造成**光晕现象**和**梯度反转**等视觉瑕疵。

目前的**色调映射算法主要分为两大类**：全局色调映射算法和局部色调映射算法。全局色调映射算法在映射的过程中只考虑像素的亮度值，不考虑像素的具体位置，对 HDR 图像中的所有像素使用同样的映射函数进行映射。局部色调映射算法在映射的过程中会根据 HDR 图像像素的邻域信息来对图像不同局部区域使用不同的映射函数。这能够增强图像的局部对比度，但是可能会出现梯度反转和光晕现象等视觉瑕疵。

#### 相关工作

##### **全局映射算法：**

1997 年，Larson[1]等人提出了一种基于直方图的色调映射算法。首先创建亮度直方图，然后计算累积概率密度函数，根据直方图和映射后亮度的最大值和最小值进行映射。

2002 年，Reinhard[2]等人提出了一种基于平均对数压缩的全局色调映射算法。该方法首先计算图像的平均对数亮度值，因为他们发现平均像素亮度总是被映射为平均场景显示范围 18%。

2003 年，Drago[3]等人提出了一种基于底数自适应的对数色调映射算法。人眼对亮度变化的感应和对数函数相似，所以采用对数映射能取得良好的效果。

2010 年，Duan[4]等人提出了一种基于线性映射和直方图均衡化的色调映射算法。该算法将线性映射和直方图均衡化相结合，既考虑了图像的像素值，又考虑到了图像的像素分布。首先用全局直方图映射来压缩动态范围，然后通过对图像进行分割，可以对图像的局部区域进行自适应调整，增强局部对比度。

##### **局部映射算法：**

1997 年，Jobson[5]等人提出了一种基于人类视觉模型的多尺度 Retinex 色调映射算法，通过消除光照影响得到反映图像本质特征的反射图像。

2002 年，Fattal [6]等人提出了一种在梯度域上处理的色调映射算法，首先计算出图像亮度分量的梯度域，然后衰减梯度域上比较大的梯度值得到修正后梯度域，最后求解泊松方程就得到了压缩后的图像。

2002 年，Reinhard[7]等人在提出基于平均对数亮度全局色调映射算法的同时提出了一种基于摄影模型的局部色调映射算法。

2002 年，Durand和 Dorsey[8]提出了一种基于快速双边滤波的色调映射算法。使用快速双边滤波器**把图像分为基本层和细节层**，**对基本层进行动态范围压缩，然后和细节层相加进行后续处理就得到了动态范围压缩后的图像。**使用双边滤波进行色调映射容易产生光晕现象和梯度反转等视觉瑕疵。2008 年，Farbman[9] 等人提出了加权最小二乘滤波器（WLS 滤波器），该滤波器是一种基于加权最小二乘框架的边缘保持滤波器，可以在保留边缘的同时提取任意尺度的细节，并且能避免光晕效应和梯度反转。2010 年，何凯明[10]等人提出了引导滤波器。2011 年，Li Xu[11]等人提出了L<sub>0</sub> 平滑滤波器，该滤波器是在L<sub>0</sub>  最小范数的约束下，对图像进行边缘保持和平滑处理。

#### 问题和动机

我们选择**复现此题目的原因**：我们小组在选题阶段遇到了比较大的困难，选题持续了两周。**在阅读英文论文的过程中，我们感到非常的痛苦**，拿到一篇论文后我们看着学习网络的架构感到毫无头绪。在遇到此问题后，我们选择在上课前问遍了到课小组，发现大部分小组都还没有选题，大都遇到了相似的困难。我们了解到一个早已开展工作的小组的目标是选择复现一个引用较少的论文，**先定一个比较简单的工作，在做的过程中不断加深认识。**该小组的同学实力较强，使我们认识到我们对自身的定位过高。非常感谢该小组同学提供的思路，使得我们可以及时开展工作。

我们从中文论文中找到了一篇介绍Tone-Mapping Operator的论文[12]，在该论文的引导下，我们选择复现2018年由Khan[13]等人提出的ATT算法（Adaptive TVI-TMO），并复现改中文论文[12]作者们提出的TWT算法（TVI-WLS-TMO）。

#### 我们的方法简介

我们尝试复现ATT、TWT算法

1. 下载数据集[^15]。
2. 实现两个算法的框架。
3. 为算法代码添加可调参功能。
4. 评估两个算法生成的图像。

### 方法的具体细节 Details of the approach



#### **经色调映射生成的LDR图片的质量评估方法**

我们使用Hojatollah Yeganeh[14]等人提出的图像质量评估方法及matlab代码

##### **结构保真度 S**

结构保真度 S 是基于结构相似性（SSIM）提出的评价标准。它通过对比 HDR 图像和色调映射后的 LDR 图像的结构信息来评价图像质量。公式如下：

![1.png (436×100) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/1.png)

其中：

- x 和 y 分别表示来自 HDR 图像和对应色调映射后的 LDR 图像的同一块局部图像区域。
- σx 是 HDR 图像局部区域块的标准差。
- σy 是 LDR 图像局部区域块的标准差。
- σxy 是两个区域块的协方差。
- C1 和 C2 是正的稳定常数。

为了获取多尺度的结构保真度，需要对图像进行多次低通滤波，产生图像金字塔，然后对每个尺度的结构保真度求加权平均：

![2.png (352×82) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/2.png)

最终，综合各尺度的结构保真度：

![3.png (216×61) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/3.png)

其中：

- L 是滤波尺度总数。
- βl是第 l 尺度的加权指数。

##### **统计自然度 N**

统计自然度 N 是基于亮度和对比度建立的模型。经使用大约3000张包含不同类型自然场景的8-bit灰度图像进行统计，发现可以分别使用高斯和 Beta 概率密度函数拟合它们的均值和标准差直方图：

![4.png (439×151) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/4.png)

结合这两个概率密度函数，求得最终的统计自然度：

![5.png (168×70) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/5.png)

其中， K 是归一化系数，使得结果在 [0, 1] 范围内。

##### 质量因子Q

质量因子 Q 将结构保真度 S 和统计自然度 N 结合起来，用单一分数评价图像的整体质量：

![6.png (270×76) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/6.png)

其中：

- a 的范围为 [0,1]，用于调整 S 和 N 的相对重要性。
- 参数 α 和 β 确定它们的灵敏度，调节到与人眼主观评价结果一致。

在实际应用中，为了使质量因子 Q 的评价结果与人眼的主观评价结果一致，调整参数的大小得到最佳值： a=0.8012, α=0.3046, β=0.7088。

#### ATT算法原理

早期的物理实验确定了人眼对于亮度适应的阈值（just noticeable differences，JNDs）。通过让观察者适应背景光照足够长的时间，然后将光照慢慢增加，直到观察者能够感知到亮度的变化，这样的亮度值对应一个 JND 的值。下图曲线(TVI曲线)刻画了不同亮度下的JND值。

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/HVS.png" alt="2" style="zoom: 67%;" />



使用下面的式子来刻画上图的两变量之间的关系

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/curse.png" alt="curse.png (533×257) (gitee.com)" style="zoom:80%;" />

下面是ATT算法流程图

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/ATT_Flowchart.png" alt="ATT_Flowchart.png (777×150) (gitee.com)" style="zoom: 80%;" />



首先将HDR图像的三个通道按照人类对色彩的感知能力划分比例，融合为亮度通道

![formula4.png (455×52) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula4.png)

然后根据TVI曲线构建亮度通道的直方图。以HDR<sub>L</sub>的最小值作为第一个直方的中心，新的直方的中心与前一个直方的中心的距离为n 个 JND 步长，直到HDR <sub>L</sub>的最大值。下式描述了此过程，与n相乘的项即为通过将亮度值带入到TVI曲线中得出的JND步长

![formula5.png (367×53) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula5.png)

直方的数目取值在 70 至 90 之间时，ATT 算法算法的效果比较好。为此，我们实现了n的动态选择，使得传入不同图片时也可以为n设置较合适的值。此函数细节见att.py

```python
def dynamically_choose_n(min_brightness, max_brightness, n=10):

    stepsize = 0.1
    within_target_range = False

    while not within_target_range:
       	length = #计算出当前n值会产生u多少直方图区间
        if length <= 90 and length >= 70:
            return bins_centers
        elif length > 90:
            n = n + stepsize
        else:
            n = n - stepsize
```

Khan的论文[16]中并没有详细地解释直方图区间的宽度如何设计。我们认定[b<sub>i</sub> , b<sub>i</sub> + n*jnd)为直方图中第i个bin的区间，n * jnd 为这个bin的width。以下图为例，我们计算出的第一个bin的区间为[0.96367, 1.62747)

![bins.png (1317×297) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/bins.png)

当亮度值较大时，bin的width也应该更大,因为在亮度较高的情况下，人眼对亮度的变化感知能力较差。

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/widths_change.png" alt="widths_change.png (550×391) (gitee.com)" style="zoom:67%;" />

在完成上述工作后，以'input_images/input_images/input_hdr/AtriumMorning.hdr'图像为例，我们就得到了该图像像素值的分布情况**f(b)**，红线为像素值为255的分界线，便于调试，**直方图已归一化**

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/brightness_channel_distribution.png" alt="brightness_channel_distribution.png (640×480) (gitee.com)" style="zoom:67%;" />



基于直方图的色调映射的一个缺点是，**当直方图中有一个较大的峰值时，对比度会增强而不是压缩**。这可能会导致图像的某些部分出现光晕现象，而其他部分则会被过度压缩。ATT算法通过将 JND 中强度非常接近的所有像素计算为一个来进行计数。这样可以减少相似型像素对显示级别的占有。首先将亮度值像素按升序排列，从先前直方中删除那些在一个JND 之中的像素，然后就得到了改进后的精细直方图 **r(b) 。**

ω取0.8，计算得出**c(b)**

![formula6.png (410×56) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula6.png)



<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/three_histogram.png" alt="three_histogram.png (770×628) (gitee.com)" style="zoom: 50%;" />

然后根据公式（7）计算累计直方图**C(b)**

![formula7.png (527×92) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula7.png)

下面是根据累计直方图C(b)计算用于像素值映射的Look Up Table的过程

 Look Up Table 共有两列，第一列放入之前得到的bins的区间的较小值，第二列放入累计直方图的值*255的结果，即可得到将HDR的像素值映射到LDR像素值的查找表

```python
def create_LUT(bins_centers, cumulative_histogram):
    LUT = np.zeros((len(bins_centers), 2))  
    LUT[:, 0] = bins_centers           
    LUT[:, 1] = cumulative_histogram * 255 
    return LUT
```

下图为LUT的一部分

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/LUT.png" alt="LUT.png (332×582) (gitee.com)" style="zoom:50%;" />

基于该表中的<HDR亮度值, LDR像素值 >对来执行线性插值，以将输入图像中的每个 HDR 值映射为相应的 LDR 值。

最后根据公式（8）进行颜色校正，将通过LUT计算出来的LDR<sub>L</sub>转变有彩色的LDR图像



![formula8.png (455×68) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula8.png)

为了将HDR像素值缩放至[0, 255),作者通过实验，Khan[16]选定s=0.67, 对于大部分图片的效果比较好，我们对此有些疑问，为什么s=0.67?因此，我们设计了颜色校正函数，使得我们可以调整参数s的值，选取出能够将 (1 - threshold) % 像素值放缩至[0, 255)的参数s, 当我们将threshold设置为0.001，s的初始值设置为1时，此函数得到的最适参数s值为0.22.当我们将threshold设为0.01,s的初始值设为1时，函数选出的s值0.89

```python
def color_correctness(hdr_image, brightness_channel, ldr_brightness_channel):
    # 颜色校正，此处有可调试参数s
    # TODO:实现检查映射情况的代码,以便调试参数s
    B, G, R = cv2.split(hdr_image)
    s = 0.67
    learning_rate = 0.1
    threshold = 0.01
    stop_condition = False
    max_iterations = 1000
    iteration = 0

    while not stop_condition and iteration < max_iterations:
        # 使用当前 s 值计算映射图像
        ldr_image_B = (B / brightness_channel) ** s * ldr_brightness_channel
        ldr_image_G = (G / brightness_channel) ** s * ldr_brightness_channel
        ldr_image_R = (R / brightness_channel) ** s * ldr_brightness_channel

        # 计算超过 255 的像素比例和最大像素值
        B_over_ratio = np.sum(ldr_image_B > 255) / ldr_image_B.size
        G_over_ratio = np.sum(ldr_image_G > 255) / ldr_image_G.size
        R_over_ratio = np.sum(ldr_image_R > 255) / ldr_image_R.size

        # 根据超过 255 的像素比例和最大像素值调整 s 值
        s -= learning_rate * (B_over_ratio + G_over_ratio + R_over_ratio) / 3

        # 更新迭代次数
        iteration += 1
        print(f"iteration: {iteration} s: {s}")

        # 检查停止条件
        if B_over_ratio < threshold and G_over_ratio < threshold and R_over_ratio < threshold:
            stop_condition = True

    ldr_image = cv2.merge((ldr_image_B, ldr_image_G, ldr_image_R))
    # 将像素值限制在 0 到 255 之间
    ldr_image = np.clip(ldr_image, 0, 255).astype(np.uint8)
    return ldr_image
```

<div style="display: flex; justify-content: space-between;">
    <figure style="margin: 0 1%; width: 23%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/prove/att_test20.jpg" alt="s=0.2" style="width: 100%;">
        <figcaption style="text-align: center;">s=0.2</figcaption>
    </figure>
    <figure style="margin: 0 1%; width: 23%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/prove/att_test40.jpg" alt="s=0.4" style="width: 100%;">
        <figcaption style="text-align: center;">s=0.4</figcaption>
    </figure>
    <figure style="margin: 0 1%; width: 23%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/prove/att_test60.jpg" alt="s=0.6" style="width: 100%;">
        <figcaption style="text-align: center;">s=0.6</figcaption>
    </figure>
    <figure style="margin: 0 1%; width: 23%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/prove/att_test80.jpg" alt="s=0.8" style="width: 100%;">
        <figcaption style="text-align: center;">s=0.8</figcaption>
    </figure>
</div>
从左到右为s值为0.2、0.4、0.6、0.8时生成的LDR图像，可以看到s=0.2时像素值被过度压缩，绿植的叶子失去了本色。我们在此次测试中发现，对于图像'input_images/input_images/input_hdr/AtriumMorning.hdr'，s取0.6-0.7时肉眼观察的视觉效果比较好。Khan选取的s=0.67是经过多次实验选出的。



#### **TWT算法原理**

TWT算法为中文论文作者程虹[12]针对 Khan[13] 等人提出的ATT算法存在的**细节保持不够**问题，提出的一种**细节保持的色调映射算法** 。该算法将采用加权最小二乘滤波进行图像基本层和细节层的分解，对基本层采用ATT算法进行亮度压缩，对细节层采用 S 曲线进行增强，合并处理后进行颜色校正。

首先将HDR的三个颜色通道转变为亮度通道***I***,然后根据公式（3.8）将亮度值扩大10^6倍，增强图像对比度，以充分利用对数域空间，然后对找出***I***的最大值，对***I***进行**归一化**

![foumula3.8.png (442×50) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/foumula3.8.png)

我们按照WLS滤波器原理实现的滤波器使用空间太大，该滤波器应用稀疏矩阵来实现

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/Overflow.png" alt="Overflow.png (1431×532) (gitee.com)" style="zoom:50%;" />

于是我们选择在python代码中调用**WLS滤波器**提出者[10]实现的matlab代码，过滤出基本层**B<sub>base</sub>**和细节层**B<sub>detail</sub>**

![formula3.10.png (426×68) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula3.10.png)

TVI 曲线在对数域进行作用的范围是 [ -4,6] ，于是我们按照公式(3.11)将**B<sub>base</sub>** 进行线性处理变换和反对数处理使之能够用人类视觉模型处理,然后将**B<sub>base</sub>**按照与ATT算法相同的逻辑，构造LUT,进行亮度压缩，得到LDR的亮度通道值二维矩阵**D<sub>base</sub>**

![formula3.11.png (428×45) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula3.11.png)

由于细节层在零附近振荡,如下图

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/histogram_B_detail.png" alt="histogram_B_detail.png (1000×500) (gitee.com)" style="zoom:50%;" />

需要用一种函数来**压缩偏离零的大偏差并增强小的偏差**。对于图像细节层**B<sub>detail</sub>**，通过 **S曲线**进行细节增强得到**D<sub>detail</sub>**

![formula3.14.png (477×50) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula3.14.png)

![S_curse.png (360×307) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/S_curse.png)

最后**将基本层和细节层加和**并进行颜色校正，颜色校正同ATT算法

![formula3.15.png (436×62) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/formula3.15.png)

但是我们根据作者[12]的逻辑实现的TWT算法效果并不好,相比ATT算法产生的图像要模糊很多，我们认识到细节层出了问题。

<div style="display: flex; justify-content: space-between;">
    <figure style="margin: 0 1%; width: 48%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/doll_author.png" alt="doll_author.png" alt="doll_auther" style="width: 100%;">
        <figcaption style="text-align: center;">TWT</figcaption>
    </figure>
    <figure style="margin: 0 1%; width: 48%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/doll_att.png" alt="doll_att.png" alt="doll_att" style="width: 100%;">
        <figcaption style="text-align: center;">ATT</figcaption>
    </figure>
</div>

经过调试和重读论文[12]，我们发现，作者的描述存在问题。

**D<sub>detail</sub>**经过S曲线增强后被归一化，那么**D<sub>detail</sub>**二维数组的值都在[-1, 1]之间，而**D<sub>base</sub>**则是经过亮度压缩像素值大约在[0 ,255]之间，**D<sub>detail</sub>**对**D<sub>base</sub>**的增强效果是**微乎其微**的，这就导致了最后生成的图像是模糊的。



面对此问题，我们讨论出**两种解决方案**。

方案一是先增强B<sub>detail</sub>得到D<sub>detail</sub>，然后将B<sub>base</sub>和D<sub>detail</sub>的和相加，以达到增强图像细节的效果,随后进行亮度压缩和颜色校正

方案二是不对D<sub>detail</sub>进行归一化,还需要扩大D<sub>detail</sub>的分布范围，以增强D<sub>detail</sub>的细节增强作用

下面是我们经过调整参数后得到的效果图

<div style="display: flex; justify-content: space-between;">
    <figure style="margin: 0 1%; width: 48%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/dolltwt_method1.png" alt="doll_author.png" alt="doll_auther" style="width: 100%;">
        <figcaption style="text-align: center;">方案一</figcaption>
    </figure>
    <figure style="margin: 0 1%; width: 48%;">
        <img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/dolltwt_method2.png" alt="doll_att" style="width: 100%;">
        <figcaption style="text-align: center;">方案二</figcaption>
    </figure>
</div>



![evaluation1.png (176×192) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/evaluation1.png)

![evaluation2.png (166×92) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/evaluation2.png)

从上到下依次是ATT算法、TWT算法方案二、TWT算法方案一的得分

通过与程虹的结果比较，我们发现我们的方案一得出了N指数更好的效果图像，我们的方案一在N指数得分与其实现差别较大，我们的ATT算法实现的比较规范。

![author_result.png (428×237) (gitee.com)](https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/author_result.png)

### 结果 Results

att、twt为程虹在论文[12]中的实现结果,ATT、TWT_m1、TWT_m2为我们的实现，TWT_m1为method1,TWT_m2为method2

tinera

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/titerna.png" alt="titerna.png (796×412) (gitee.com)" style="zoom:50%;" />

doll

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/玩偶.png" alt="玩偶.png (801×399) (gitee.com)" style="zoom:50%;" />

memorial

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/教堂.png" alt="教堂.png (819×429) (gitee.com)" style="zoom:50%;" />

vinesunset

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/夕阳.png" alt="夕阳.png (828×414) (gitee.com)" style="zoom:50%;" />

HancockKitchen

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/厨房.png" alt="厨房.png (799×403) (gitee.com)" style="zoom:50%;" />

cadik

<img src="https://gitee.com/qxzdgitee/cv_-course_-resource/raw/master/final-project/书桌.png" alt="书桌.png (807×408) (gitee.com)" style="zoom:50%;" />



在图像memorial、vinesunset、HancockKitchen、cadik中，可以看到我们的**TWT method1可以得到更高的N项得分**

### 总结和讨论 Discussion and conclusions

在此次实验中，我们形成了根据直方图来调试生成图像质量的方法（具体演示可见展示视频）。我们通过将中间结果存储来加速调试参数的过程，避免重复计算。在复现过程中，我们遇到了TWT算法不能达到程虹实验效果的情况，经过分析，我们认为这是作者的问题，在作者的思路的基础上，我们实现了TWT_method1和TWT_method2。在[16]中，作者提到不利用学习方法进行色调映射需要实验人员手动调试参数，图像生成效率低。我们在复现过程中深深的感受到了这一点。起初，为了使我们的project尽快地启动起来，我们没有使用机器学习和深度学习的方法，但在实验过程中，我们体会到了学习方法校正参数的高效性。

### 个人贡献声明 Statement of individual contribution

- 乔贤争(60%)：数据获取，代码编写及测试，统筹规划任务安排
- 赵伟凌(20%)：论文选择，代码编写
- 马倩怡(20%)：代码编写，项目报告编辑


### 引用参考 References

[1]: Larson G W, Rushmeier H, Piatko C. A Visibility Matching Tone Reproduction Operator for High Dynamic Range Scenes[J]. IEEE Transactions on Visualization and Computer Graphics, 1997, 3(4):291-306.

[2]: Reinhard E, Stark M, Shirley P, et al. Photographic tone reproduction for digital images[J]. ACM Transactions on Graphics, 2002, 21(3):267-276.

[3]: Drago F, Myszkowski K, Annen T, et al. Adaptive logarithmic mapping for displaying high contrast scenes [C]. In Computer Graphics Forum. 2003:419-426.

[4]: Duan J, Bressan M, Dance C, et al. Tone-mapping high dynamic range images by novel histogram adjustment [J]. Pattern Recognition. 2010, 43 (5):1847-1862.

[5]: Jobson D J, Rahman Z, Woodell G A. Properties and performance of a center/surround retinex[J]. IEEE Transactions on Image Processing, 1997, 6(3):451-462.

[6]: Fattal R, Lischinski D, Werman M. Gradient domain high dynamic range compression[J]. In ACM Transactions on Graphics, 2002, 21(3):249-256.

[7]: Reinhard E, Stark M, Shirley P, et al. Photographic tone reproduction for digital images[J]. ACM Transactions on Graphics, 2002, 21(3):267-276.

[8]: Durand F, Dorsey J. Fast bilateral filtering for the display of high-dynamic-range images[J]. ACM Transactions on Graphics, 2002, 21(3):257-266.

[9]: Farbman Z, Fattal R, Lischinski D, et al. Edge-preserving decompositions for multi-scale tone and detail manipulation[J]. ACM Transactions on Graphics, 2008, 27(3):1.

[10]: Kaiming H, Jian S, Xiaoou T. Guided Image Filtering[C]. European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010.

[11]: Xu L, Lu C, Xu Y, et al. Image smoothing via L0 gradient minimization[J]. ACM Transactions on Graphics, 2011, 30(6):1-12.

[12]: 程虹. 高动态范围图像的色调映射算法研究[D]. 中国科学院大学 (中国科学院光电技术研究所), 2019.

[13]: Khan I R, Rahardja S, Khan M M, et al. A Tone-Mapping Technique Based on Histogram Using a Sensitivity Model of the Human Visual System[J]. IEEE Transactions on Industrial Electronics, 2018, 65(4):3469-3479.

[14]: Yeganeh H, Wang Z. Objective quality assessment of tone-mapped images[J]. IEEE Transactions on Image Processing, 2013, 22(2):657-667.

[16]: Panetta K, Kezebou L, Oludare V, Agaian S, Xia Z. TMO-Net: A Parameter-Free Tone Mapping Operator Using Generative Adversarial Network, and Performance Benchmarking on Large Scale HDR Dataset. IEEE Access, 2021, 9:39500-39517. doi:10.1109/ACCESS.2021.3064295.
